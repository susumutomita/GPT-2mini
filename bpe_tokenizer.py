#!/usr/bin/env python3
"""
BPE (Byte Pair Encoding) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…

Character-level ã¨ã®é•ã„ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®æ•™è‚²ç”¨å®Ÿè£…ã€‚

BPE ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
1. åˆæœŸèªå½™ = å…¨ã¦ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ–‡å­—ï¼ˆã¾ãŸã¯ãƒã‚¤ãƒˆï¼‰
2. ãƒ†ã‚­ã‚¹ãƒˆä¸­ã§æœ€ã‚‚é »å‡ºã™ã‚‹é€£ç¶šãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
3. ãã®ãƒšã‚¢ã‚’æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ãƒãƒ¼ã‚¸
4. ç›®æ¨™ã®èªå½™ã‚µã‚¤ã‚ºã«ãªã‚‹ã¾ã§ 2-3 ã‚’ç¹°ã‚Šè¿”ã™
"""

import re
import sys
from collections import Counter
from typing import Dict, List, Tuple


class BPETokenizer:
    """
    Byte Pair Encoding ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

    Usage:
        tokenizer = BPETokenizer()
        tokenizer.train(text, vocab_size=1000)
        tokens = tokenizer.encode("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹")
        text = tokenizer.decode(tokens)
    """

    def __init__(self):
        self.vocab: Dict[int, bytes] = {}  # id -> bytes
        self.merges: Dict[Tuple[int, int], int] = {}  # (id1, id2) -> new_id
        self.vocab_size = 256  # åˆæœŸã¯å…¨ãƒã‚¤ãƒˆ

    def _get_stats(self, ids: List[int]) -> Counter:
        """é€£ç¶šã™ã‚‹ãƒšã‚¢ã®å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        pairs = Counter()
        for i in range(len(ids) - 1):
            pairs[(ids[i], ids[i + 1])] += 1
        return pairs

    def _merge(self, ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:
        """æŒ‡å®šã—ãŸãƒšã‚¢ã‚’æ–°ã—ã„ ID ã«ãƒãƒ¼ã‚¸"""
        new_ids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:
                new_ids.append(new_id)
                i += 2
            else:
                new_ids.append(ids[i])
                i += 1
        return new_ids

    def train(self, text: str, vocab_size: int = 1000, verbose: bool = True):
        """
        BPE ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

        Args:
            text: å­¦ç¿’ãƒ†ã‚­ã‚¹ãƒˆ
            vocab_size: ç›®æ¨™èªå½™ã‚µã‚¤ã‚ºï¼ˆ256 ä»¥ä¸Šï¼‰
            verbose: å­¦ç¿’éç¨‹ã‚’è¡¨ç¤º
        """
        assert vocab_size >= 256, "vocab_size must be >= 256 (base bytes)"

        # åˆæœŸåŒ–: å…¨ãƒã‚¤ãƒˆã‚’èªå½™ã«è¿½åŠ 
        self.vocab = {i: bytes([i]) for i in range(256)}
        self.merges = {}

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›ã—ã€ID ãƒªã‚¹ãƒˆã«
        text_bytes = text.encode("utf-8")
        ids = list(text_bytes)

        if verbose:
            print(f"å­¦ç¿’é–‹å§‹: {len(text)} æ–‡å­— â†’ {len(ids)} ãƒã‚¤ãƒˆ")
            print(f"ç›®æ¨™èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
            print("-" * 50)
            sys.stdout.flush()

        num_merges = vocab_size - 256
        for i in range(num_merges):
            # æœ€é »å‡ºãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
            stats = self._get_stats(ids)
            if not stats:
                break

            top_pair = stats.most_common(1)[0][0]
            freq = stats[top_pair]

            # é »åº¦ãŒ 1 ä»¥ä¸‹ãªã‚‰çµ‚äº†
            if freq <= 1:
                if verbose:
                    print(f"ã“ã‚Œä»¥ä¸Šãƒãƒ¼ã‚¸ã™ã‚‹ãƒšã‚¢ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{i} å›ã§ã‚¹ãƒˆãƒƒãƒ—ï¼‰")
                break

            # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆ
            new_id = 256 + i
            self.merges[top_pair] = new_id
            self.vocab[new_id] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]

            # ãƒãƒ¼ã‚¸ã‚’é©ç”¨
            ids = self._merge(ids, top_pair, new_id)

            if verbose and (i + 1) % 50 == 0:
                token_str = self.vocab[new_id].decode("utf-8", errors="replace")
                print(f"ãƒãƒ¼ã‚¸ {i+1}/{num_merges}: '{token_str}' (é »åº¦: {freq}, æ®‹ã‚Š: {len(ids)} ãƒˆãƒ¼ã‚¯ãƒ³)")
                sys.stdout.flush()

        self.vocab_size = len(self.vocab)
        if verbose:
            print("-" * 50)
            print(f"å­¦ç¿’å®Œäº†: èªå½™ã‚µã‚¤ã‚º = {self.vocab_size}")
            # åœ§ç¸®ç‡ã‚’è¨ˆç®—
            original_len = len(text.encode("utf-8"))
            compressed_len = len(ids)
            print(f"åœ§ç¸®ç‡: {original_len} â†’ {compressed_len} ({compressed_len/original_len*100:.1f}%)")
            sys.stdout.flush()

    def encode(self, text: str) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ ID ãƒªã‚¹ãƒˆã«å¤‰æ›"""
        ids = list(text.encode("utf-8"))

        # å­¦ç¿’ã—ãŸãƒãƒ¼ã‚¸ã‚’é †ç•ªã«é©ç”¨
        while len(ids) >= 2:
            stats = self._get_stats(ids)
            # ãƒãƒ¼ã‚¸å¯èƒ½ãªãƒšã‚¢ã®ä¸­ã§ã€æœ€ã‚‚æ—©ãå­¦ç¿’ã—ãŸã‚‚ã®ã‚’é©ç”¨
            pair = min(
                stats.keys(),
                key=lambda p: self.merges.get(p, float("inf"))
            )
            if pair not in self.merges:
                break
            new_id = self.merges[pair]
            ids = self._merge(ids, pair, new_id)

        return ids

    def decode(self, ids: List[int]) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³ ID ãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        text_bytes = b"".join(self.vocab[id] for id in ids)
        return text_bytes.decode("utf-8", errors="replace")

    def get_vocab_examples(self, n: int = 20) -> List[Tuple[int, str]]:
        """èªå½™ã®ä¾‹ã‚’è¡¨ç¤ºï¼ˆãƒãƒ¼ã‚¸ã§ä½œã‚‰ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ï¼‰"""
        examples = []
        for id in range(256, min(256 + n, self.vocab_size)):
            token_bytes = self.vocab[id]
            token_str = token_bytes.decode("utf-8", errors="replace")
            examples.append((id, token_str))
        return examples


def main():
    """BPE ã®å‹•ä½œãƒ‡ãƒ¢"""
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    with open("data.txt", "r", encoding="utf-8") as f:
        text = f.read()

    print("=" * 60)
    print("BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‡ãƒ¢")
    print("=" * 60)
    print()

    # BPE ã‚’å­¦ç¿’
    tokenizer = BPETokenizer()
    tokenizer.train(text, vocab_size=2000, verbose=True)

    print()
    print("=== ãƒãƒ¼ã‚¸ã§ä½œã‚‰ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ä¾‹ ===")
    for id, token in tokenizer.get_vocab_examples(30):
        print(f"  {id}: '{token}'")

    print()
    print("=== ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ ===")
    test_texts = [
        "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹",
        "ç§ã®åå‰ã¯ç”°ä¸­ã§ã™",
        "Hello World",
        "é¾",  # Character-level ã§ã¯æ‰±ãˆãªã‹ã£ãŸ
        "ğŸ˜€",  # çµµæ–‡å­—
    ]

    for t in test_texts:
        ids = tokenizer.encode(t)
        decoded = tokenizer.decode(ids)
        print(f"  '{t}' â†’ {len(ids)} ãƒˆãƒ¼ã‚¯ãƒ³ â†’ '{decoded}'")


if __name__ == "__main__":
    main()
