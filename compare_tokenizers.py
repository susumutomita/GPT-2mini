#!/usr/bin/env python3
"""
Character-level ã¨ BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ¯”è¼ƒ

Usage:
    python compare_tokenizers.py
"""

import torch
from train_gpt2_mini import CharDataset, GPT2Mini, GPTConfig
from train_gpt2_bpe import BPEDataset, GPT2Mini as GPT2MiniBPE


def get_device():
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    return "cpu"


def main():
    device = get_device()
    print("=" * 70)
    print("Character-level vs BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ¯”è¼ƒ")
    print("=" * 70)
    print()

    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_texts = [
        "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹",
        "ç§ã¯å­¦ç”Ÿã§ã™",
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­",
        "Hello, World!",
        "é¾",  # Character-level ã§ã¯æœªçŸ¥æ–‡å­—
        "ğŸ˜€",  # çµµæ–‡å­—
    ]

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open("data.txt", "r", encoding="utf-8") as f:
        text = f.read()

    print("=== ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ¯”è¼ƒ ===")
    print()

    # Character-level ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    print("ã€Character-levelã€‘")
    char_dataset = CharDataset(text[:10000], block_size=256)  # å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã§åˆæœŸåŒ–
    print(f"  èªå½™ã‚µã‚¤ã‚º: {char_dataset.vocab_size}")
    print()

    # BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ï¼‰
    print("ã€BPEã€‘")
    checkpoint = torch.load("model_bpe.pt", map_location=device, weights_only=False)
    bpe_tokenizer = checkpoint["tokenizer"]
    print(f"  èªå½™ã‚µã‚¤ã‚º: {bpe_tokenizer.vocab_size}")
    print()

    print("=== ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¾‹ ===")
    print()

    for text_sample in test_texts:
        print(f'å…¥åŠ›: "{text_sample}"')
        print("-" * 50)

        # Character-level
        try:
            char_tokens = [char_dataset.stoi[c] for c in text_sample]
            char_decoded = "".join(char_dataset.itos[t] for t in char_tokens)
            print(f"  Char: {len(char_tokens)} ãƒˆãƒ¼ã‚¯ãƒ³ â†’ {char_tokens[:10]}...")
            print(f"  å¾©å…ƒ: {char_decoded}")
        except KeyError as e:
            print(f"  Char: ã‚¨ãƒ©ãƒ¼ - æœªçŸ¥æ–‡å­— {e}")

        # BPE
        bpe_tokens = bpe_tokenizer.encode(text_sample)
        bpe_decoded = bpe_tokenizer.decode(bpe_tokens)
        print(f"  BPE:  {len(bpe_tokens)} ãƒˆãƒ¼ã‚¯ãƒ³ â†’ {bpe_tokens[:10]}...")
        print(f"  å¾©å…ƒ: {bpe_decoded}")

        print()

    print("=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===")
    print()

    # Character-level ãƒ¢ãƒ‡ãƒ«
    print("ã€Character-level ãƒ¢ãƒ‡ãƒ« (model_v2.pt)ã€‘")
    char_checkpoint = torch.load("model_v2.pt", map_location=device, weights_only=False)
    char_config = char_checkpoint["config"]
    print(f"  èªå½™ã‚µã‚¤ã‚º: {char_config.vocab_size}")
    print(f"  ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {char_config.n_layer}")
    print(f"  ãƒ˜ãƒƒãƒ‰æ•°: {char_config.n_head}")
    print(f"  åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {char_config.n_embd}")
    print(f"  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {char_config.block_size}")

    # BPE ãƒ¢ãƒ‡ãƒ«
    print()
    print("ã€BPE ãƒ¢ãƒ‡ãƒ« (model_bpe.pt)ã€‘")
    bpe_config = checkpoint["config"]
    print(f"  èªå½™ã‚µã‚¤ã‚º: {bpe_config.vocab_size}")
    print(f"  ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {bpe_config.n_layer}")
    print(f"  ãƒ˜ãƒƒãƒ‰æ•°: {bpe_config.n_head}")
    print(f"  åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {bpe_config.n_embd}")
    print(f"  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {bpe_config.block_size}")

    print()
    print("=== ç”Ÿæˆæ¯”è¼ƒ ===")
    print()

    prompt = "å¾è¼©ã¯"

    # Character-level ç”Ÿæˆ
    print(f"ã€Character-levelã€‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
    full_char_dataset = CharDataset(text, block_size=char_config.block_size)
    char_model = GPT2Mini(char_config).to(device)
    char_model.load_state_dict(char_checkpoint["model"])
    char_model.train(False)

    idx = torch.tensor([[full_char_dataset.stoi[c] for c in prompt]], dtype=torch.long, device=device)
    with torch.no_grad():
        generated = char_model.generate(idx, max_new_tokens=100, temperature=0.8, top_k=40)
    char_output = "".join(full_char_dataset.itos[t] for t in generated[0].tolist())
    print(char_output)
    print()

    # BPE ç”Ÿæˆ
    print(f"ã€BPEã€‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
    bpe_model = GPT2MiniBPE(bpe_config).to(device)
    bpe_model.load_state_dict(checkpoint["model"])
    bpe_model.train(False)

    idx = torch.tensor([bpe_tokenizer.encode(prompt)], dtype=torch.long, device=device)
    with torch.no_grad():
        generated = bpe_model.generate(idx, max_new_tokens=100, temperature=0.8, top_k=40)
    bpe_output = bpe_tokenizer.decode(generated[0].tolist())
    print(bpe_output)
    print()

    print("=== ã¾ã¨ã‚ ===")
    print()
    print("| é …ç›® | Character-level | BPE |")
    print("|------|-----------------|-----|")
    print(f"| èªå½™ã‚µã‚¤ã‚º | {char_config.vocab_size} | {bpe_config.vocab_size} |")
    print(f"| æœªçŸ¥æ–‡å­—å¯¾å¿œ | Ã— | â—‹ |")
    print(f"| æ—¥æœ¬èªã®æ–‡å­—åŒ–ã‘ | ãªã— | ã‚ã‚Šï¼ˆãƒã‚¤ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ |")
    print(f"| ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ | 1æ–‡å­—=1ãƒˆãƒ¼ã‚¯ãƒ³ | åœ§ç¸®ã‚ã‚Š |")


if __name__ == "__main__":
    main()
